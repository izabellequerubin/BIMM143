---
title: "Class 08 Mini-Project"
author: "Izabelle Querubin"
format: pdf
---

# Class 8 Mini-Project: Unsupervised Learning Analysis of Human Breast Cancer Cells

## 1. Preparing the Data

```{r}
# Save you input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)

```

```{r}
wisc.data <- wisc.df[,-1]
```

```{r}
# Create diagnosis vector for later 

# Extract diagnosis column
diagnosis <- as.factor(wisc.df$diagnosis)
```

### Q1. How many observations are in this dataset?

```{r}
nrow(wisc.data)
```

There are 569 observations in this dataset.

### Q2. How many of the observations have a malignant diagnosis?

```{r}
table(diagnosis)
```

212 of the observations have a malignant diagnosis.

### Q3. How many variables/features in the data are suffixed with `_mean`?

```{r}
length(grep("_mean", colnames(wisc.data)))
```

10 variable/features are suffixed with \_mean.

## 2. Principal Component Analysis

```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data, 2, sd)
```

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(scale(wisc.data))
```

```{r}
summary(wisc.pr)
```

### Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

```{r}
pca_var <- wisc.pr$sdev^2 # extract the eigenvalues
prop_var <- pca_var/sum(pca_var) # calculate the proportion of variance
prop_var[1] # print the proportion of variance captured by PC1
```

44% of the original variance is capture by PC1.

### Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

```{r}
cum_prop_var <- cumsum(prop_var) # calculate cumulative proportion of variance
which.min(cum_prop_var < 0.7) + 1 # print the number of PCs required to explain at least 70% of variance
```

4 PCs are required.

### Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

```{r}
which.min(cum_prop_var < 0.9) + 1 # print the number of PCs required to explain at least 90% of variance
```

8 PCs are required.

### Interpreting PCA Results

```{r}
biplot(wisc.pr)
```

### Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

This plot is very messy, making it extremely difficult to understand.

```{r}
# Scatter plot observations by components 1 and 2
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis,
     xlab = "PC1", ylab = "PC2")
```

### Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = diagnosis,
     xlab = "PC1", ylab = "PC3")
```

Compared to PC3, PC2 does a much better job at cleanly separating the different subgroups; therefore, the first plot is the preferred plot.

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) +
  aes(PC1, PC2, col = diagnosis) +
  geom_point()
```

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note dat driven y-axis
barplot(pve, ylab = "Percent of Variance Explained",
        names.arg = paste0("PC", 1:length(pve)), las = 2, axes = FALSE)
axis(2, at = pve, labels = round(pve,2)*100)
```

```{r}
## ggplot based graph
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

### Communicating PCA Results

### Q9. For the first principal component, what is the component of the loading vector (i.e. `wisc.pr$rotation[,1]`) for the feature `concave.points_mean`? This tells us how much this original feature contributes to the first PC.

```{r}
wisc.pr$rotation["concave.points_mean", 1]
```

## 3. Hierarchical Clustering

```{r}
data.scaled <- scale(wisc.data)
```

```{r}
data.dist <- dist(data.scaled)
```

```{r}
wisc.hclust <- hclust(data.dist, method = "complete")
```

### Q10. Using the `plot()` and `abline()` functions, what is the height at the which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust, main="Dendrogram of Hierarchical Clustering")
abline(h=4, col="red", lty=2)
```

### Selecting number of clusters

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
```

```{r}
table(wisc.hclust.clusters, diagnosis)
```

### Using different methods

### Q12. Which method gives your favorite results for the same `data.dist` dataset? Explain your reasoning. 

I don't think I have favorite method, per se, but I appreciate the results of "average" linkage since it is a compromise of the "single" and "complete" linkages, and is very applicable to many data sets.

## 4. Combining methods

### Clustering on PCA results

```{r}
# Calculate cumulative variance explained by each principal component
cumulative_var <- cumsum(wisc.pr$sdev^2) / sum(wisc.pr$sdev^2)

# Find the minimum number of principal components required to explain 90% of the variability
num_components <- min(which(cumulative_var >= 0.9))

# Create hierarchical clustering model with linkage method="ward.D2"
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:num_components]), method="ward.D2")
```

```{r}
plot(wisc.pr.hclust, main="Dendrogram of Hierarchical Clustering")
```

```{r}
grps <- cutree(wisc.pr.hclust, k = 2)
table(grps)
```

```{r}
table(grps, diagnosis)
```

```{r}
plot(wisc.pr$x[,1:2], col = grps)
```

```{r}
plot(wisc.pr$x[,1:2], col = diagnosis)
```

```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```

```{r}
# Plot using our re-ordered factor
plot(wisc.pr$x[,1:2], col = g)
```

```{r}
# Calculate cumulative variance explained by each principal component
cumulative_var <- cumsum(wisc.pr$sdev^2) / sum(wisc.pr$sdev^2)

# Find the minimum number of principal components required to explain 90% of the variability
min_components <- min(which(cumulative_var >= 0.9))

# Create hierarchical clustering model with linkage method="ward.D2"
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:min_components]), method="ward.D2")
```

```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```

### Q13. How well does the newly created model with four clusters separate out the two diagnoses?

```{r}
# Compare to actual diagnoses
table(wisc.pr.hclust.clusters, diagnosis)
```

The new model works efficiently to separate the two diagnoses from the four clusters.

### Q14. How well do the hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? 

```{r}
table(wisc.hclust.clusters, diagnosis)
```

They do fine but they require more work/code/math to be done, while the newer model does not.

## 6. Prediction

```{r}
url <- "new_samples.csv"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

### Q16. Which of these new patients should we prioritize for follow up based on your results?

For some reason, this plot does not match the plot shown in the lab. For the purposes of this question, I will base my results on the plot in the lab. Patient 2 (blue dot #2) should be prioritized for follow up. They are an outlier compared to the other patients, which are clustered together near zero on the first principal component. Because of this, patient 2 should be a priority for follow-up as they may have a higher risk of malignancy or require further investigation.
